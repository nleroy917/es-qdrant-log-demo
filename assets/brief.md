# Query latency under write contention: Qdrant vs. Elasticsearch for hybrid vector log search

## Abstract

Modern observability platforms are increasingly adopting vector search to enable semantic log retrieval alongside traditional keyword matching. However, log databases must simultaneously handle high-throughput writes from distributed microservices while serving low-latency search queries to site reliability engineers. We benchmark Elasticsearch and Qdrant for hybrid vector log search (dense embeddings + BM25) under sustained write contention, measuring query latency degradation and recovery characteristics across three phases: steady-state, heavy-write, and post-write recovery. Our results show that Qdrant maintains near-baseline query performance under heavy write load, while Elasticsearch exhibits significant latency degradation with slow recovery.

## Introduction

Log search is a foundational capability in modern software operations. When a production incident occurs, site reliability engineers (SREs) must quickly locate relevant log entries across thousands of microservices producing millions of log lines per hour. Traditional log search relies on keyword matching and structured filtering — systems like the Elastic Stack (ELK) have dominated this space for over a decade.

The emergence of dense vector embeddings has introduced a new dimension to log search. By encoding log messages into high-dimensional vector representations using language models, operators can perform *semantic* searches — finding logs that are conceptually related to a query even when they share no exact keywords. Hybrid search, which combines dense vector similarity with sparse keyword matching (e.g., BM25), offers the best of both approaches.

However, log databases face a challenge that many vector search benchmarks ignore: **continuous, bursty write load**. Unlike static document collections, log indices are under constant pressure from incoming data. This shifts the experimental setup from *how fast can the system search*, to *how does search performance degrade when the system is simultaneously ingesting data at scale*?

In this brief study, we compare Elasticsearch (the most widely deployed Lucene-based search engine) and Qdrant (a purpose-built vector database) for hybrid log search under write contention. We design a controlled benchmark that measures query latency across three phases — baseline, heavy-write, and recovery — to characterize each system's behavior under realistic operational conditions.

## Experimental Setup

### Architecture

Our benchmark architecture consists of three components (Figure 1):

1. **A synthetic log emitter** that generates realistic log entries from simulated microservices at configurable rates
2. **Two log data stores** — Elasticsearch and Qdrant — receiving identical log streams simultaneously
3. **A query load generator** ([qstorm](https://github.com/nleroy917/qstorm)) that continuously executes hybrid search queries against both backends and records per-burst latency statistics

<p align="center">
  <img src="overview.svg" alt="Figure 1: Experimental architecture" width="100%"/>
</p>

*Figure 1.* **(a)** High-level architecture: microservices write logs to both data stores while SREs issue read queries. **(b)** Benchmark phases and expected measurement profiles. **(c)** The log emitter generates synthetic entries, buffers them, and flushes to configurable sinks.

### Dataset

Each log entry consists of a unique identifier, timestamp, service name, log level (Debug, Info, Warn, Error), a natural-language message, and a 1,536-dimensional dense embedding generated by OpenAI's `text-embedding-3-small` model. Log messages are drawn from a pre-generated pool of 10,000 unique messages covering common operational scenarios (connection errors, resource exhaustion, authentication failures, etc.), with level distributions configured per service to simulate realistic log patterns.

Four simulated microservices generate logs at a combined rate of ~420 logs/second:

| Service | Rate (logs/s) | Level distribution |
|---------|--------------|-------------------|
| api-gateway | 300 | 70% Info, 15% Warn, 10% Debug, 5% Error |
| auth-service | 50 | 60% Info, 20% Warn, 15% Error, 5% Debug |
| user-service | 40 | 65% Info, 15% Warn, 10% Debug, 10% Error |
| payment-service | 30 | 50% Info, 25% Warn, 20% Error, 5% Debug |

### Indexing Configuration

Both systems are configured for hybrid search:

- **Elasticsearch**: The `logs` index uses a `dense_vector` field (1,536 dimensions, HNSW) for semantic search and a `text` field with the default BM25 analyzer for keyword matching. No quantization is applied.
- **Qdrant**: The `logs` collection uses a named dense vector (`dense`, 1,536 dimensions, cosine distance, HNSW) and a named sparse vector (`bm25`) with IDF modifier for keyword matching. Payload indices are created on `level` and `service` for filtered queries.

### Query Workload

The query load generator executes 45 predefined natural-language queries representative of real incident investigation (e.g., *"database connection pool exhausted"*, *"JWT token verification failed"*, *"container out of memory kill risk"*). Each query is embedded at startup and issued as a hybrid search combining dense vector similarity with BM25 keyword matching. Queries are fired in bursts of 100 with concurrency of 10, and per-burst latency percentiles (p50, p95, p99) and throughput (QPS) are recorded as JSONL.

### Benchmark Phases

The benchmark proceeds through three timed phases while query load runs continuously:

1. **Steady-state** (60 s) — No write load. Establishes baseline query latency.
2. **Heavy-write** (180 s) — The emitter is started, writing ~420 logs/second simultaneously to both backends. Measures latency degradation under write contention.
3. **Recovery** (120 s) — The emitter is stopped. Measures how quickly each system returns to baseline latency.

Prior to measurement, both databases are pre-seeded with 100,000 log entries to simulate a warm, populated index.

## Results

<!-- TODO: Insert results figures and analysis here -->

<!-- Suggested subsections:
### Query Throughput (QPS)
### Mean Latency
### Tail Latency (p95, p99)
### Recovery Characteristics
-->

## Discussion

<!-- TODO -->

## Conclusion

<!-- TODO -->